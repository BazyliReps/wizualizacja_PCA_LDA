# str(heart)
summary(heart)
# podział zbioru na treningowy i testowy
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train[,2:45] = scale(train[,2:45])
test[,2:45] = scale(test[,2:45])
library(MASS)
lnr <- lda(heart~., train)
lnr
heart <- read.csv("heart.csv")
# head(heart, 1)
# str(heart)
summary(heart)
# podział zbioru na treningowy i testowy
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train[,2:45] = scale(train[,2:45])
test[,2:45] = scale(test[,2:45])
library(MASS)
lnr <- lda('OVERALL DIAGNOSIS'~., train)
lnr
install.packages('caTools')
heart <- read.csv("heart.csv")
# head(heart, 1)
# str(heart)
summary(heart)
# podział zbioru na treningowy i testowy
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train[,2:45] = scale(train[,2:45])
test[,2:45] = scale(test[,2:45])
View(train)
heart <- read.csv("heart.csv")
# head(heart, 1)
# str(heart)
summary(heart)
# podział zbioru na treningowy i testowy
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train[,2:45] = scale(train[,2:45])
test[,2:45] = scale(test[,2:45])
library(MASS)
lnr <- lda(OVERALL_DIAGNOSIS~., train)
lnr
lnr
heart <- read.csv("heart.csv")
# head(heart, 1)
# str(heart)
summary(heart)
# podział zbioru na treningowy i testowy
set.seed(123)
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train[,2:45] = scale(train[,2:45])
test[,2:45] = scale(test[,2:45])
library(MASS)
lnr <- lda(OVERALL_DIAGNOSIS~., train)
lnr
lnr$prior
lnr$counts
pred <- predict(lnr, train)
ldahist(data = pred$x[,1], g = train$OVERALL_DIAGNOSIS)
library(devtools)
library(ggord)
install_github("fawda123/ggord")
# install_github("fawda123/ggord")
library(ggord)
ggord(lnr, train$OVERALL_DIAGNOSIS, ylim = c(-5, 5))
ggord(lnr, train$OVERALL_DIAGNOSIS)
heart <- read.csv("heart.csv")
# head(heart, 1)
# str(heart)
summary(heart)
heart <- read.csv("heart.csv")
# podział zbioru na treningowy i testowy
set.seed(123)
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train[,2:45] = scale(train[,2:45])
test[,2:45] = scale(test[,2:45])
library(MASS)
lnr <- lda(OVERALL_DIAGNOSIS~., train)
pred <- predict(lnr, train)
ldahist(data = pred$x[,1], g = train$OVERALL_DIAGNOSIS)
ldahist(data = pred$x[,1], g = train$OVERALL_DIAGNOSIS)
ldahist(data = pred$x[,1], g = train$OVERALL_DIAGNOSIS)
g = train$OVERALL_DIAGNOSIS
data = pred$x[,1]
library(devtools)
# install_github("fawda123/ggord")
library(ggord)
ggord(lnr, train$OVERALL_DIAGNOSIS)
heart <- read.csv("heart.csv")
# podział zbioru na treningowy i testowy
set.seed(123)
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train[,2:45] = scale(train[,2:45])
test[,2:45] = scale(test[,2:45])
library(MASS)
lnr <- lda(OVERALL_DIAGNOSIS~., train)
pred <- predict(lnr, train)
ldahist(data = pred$x[,1], g = train$OVERALL_DIAGNOSIS)
dev.off()
heart <- read.csv("heart.csv")
heart.pr <- prcomp(heart[c(2:length(colnames(heart)))], center = TRUE, scale = TRUE)
# wyznaczenie wariancji zawartej w dwóch największych składnikach wiodących
pc1_var <- as.double(summary(heart.pr)$importance[,1][2])
pc2_var <- as.double(summary(heart.pr)$importance[,2][2])
print(sprintf('Wariancja danych zawarta w pierwszym składniku wiodącym: %s%%', format(round(pc1_var, 2), nsmall = 2)))
print(sprintf('Wariancja danych zawarta w drugim składniku wiodącym: %s%%', format(round(pc2_var, 2), nsmall = 2)))
# Wykres wartości własnych dla 15 największych składników wiodących
pdf('docs/wartosci_wlasne.pdf')
screeplot(heart.pr, type = "l", npcs = 15, main = "Wykres wartości własnych dla 15 największych składników wiodących")
abline(h = 1, col="red", lty=5)
legend("topright", legend=c("Wartość własna = 1"),
col=c("red"), lty=5, cex=0.6)
dev.off()
# kumulacyjny wykres wariancji
cumpro <- cumsum(heart.pr$sdev^2 / sum(heart.pr$sdev^2))
pdf('docs/wariancja_kum.pdf')
plot(cumpro[0:45], xlab = "Składnik wiodący", ylab = "Wariancja wyjaśniona", main = "Kumulatywny wykres wariancji")
legend("topleft", legend=c("Cut-off @ PC6"),
col=c("blue"), lty=5, cex=0.6)
dev.off()
x_label <- sprintf('składnik wiodący 1: (%s%%)', format(round(pc1_var, 2), nsmall = 2))
y_label <- sprintf('składnik wiodący 2: (%s%%)', format(round(pc2_var, 2), nsmall = 2))
plot_title <- 'Dane po redukcji rozmiaru do dwóch wymiarów'
plot(heart.pr$x[,1],heart.pr$x[,2], xlab=x_label, ylab = y_label, main = plot_title)
# to wrzuććie do konsoli, ze skryptu mi się nie rysuje a ładne jest
library("factoextra")
pdf('docs/klasy.pdf')
fviz_pca_ind(heart.pr, geom.ind = "point", pointshape = 21,
pointsize = 2,
fill.ind = heart$OVERALL_DIAGNOSIS,
col.ind = "black",
palette = "jco",
addEllipses = TRUE,
label = "var",
col.var = "black",
repel = TRUE,
legend.title = "Diagnoza") +
ggtitle("Dane z zaznaczonymi klasami") +
theme(plot.title = element_text(hjust = 0.5))
dev.off()
source('~/Desktop/wizualizacja_PCA_LDA/proj/analiza_PCA.R')
heart <- read.csv("heart.csv")
# podział zbioru na treningowy i testowy
set.seed(123)
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train[,2:45] = scale(train[,2:45])
test[,2:45] = scale(test[,2:45])
library(MASS)
lnr <- lda(OVERALL_DIAGNOSIS~., train)
pred <- predict(lnr, train)
ldahist(data = pred$x[,1], g = train$OVERALL_DIAGNOSIS)
dev.off()
g = train$OVERALL_DIAGNOSIS
data = pred$x[,1]
library(devtools)
# install_github("fawda123/ggord")
library(ggord)
ggord(lnr, train$OVERALL_DIAGNOSIS)
ldahist(data = pred$x[,1], g = train$OVERALL_DIAGNOSIS)
heart <- read.csv("heart.csv")
# podział zbioru na treningowy i testowy
set.seed(123)
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
library(MASS)
lnr <- lda(OVERALL_DIAGNOSIS~., train)
pred <- predict(lnr, train)
ldahist(data = pred$x[,1], g = train$OVERALL_DIAGNOSIS)
dev.off()
g = train$OVERALL_DIAGNOSIS
data = pred$x[,1]
# podział zbioru na treningowy i testowy
set.seed(123)
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
library(MASS)
lnr <- lda(OVERALL_DIAGNOSIS~., train)
pred <- predict(lnr, train)
ldahist(data = pred$x[,1], g = train$OVERALL_DIAGNOSIS)
dev.off()
g = train$OVERALL_DIAGNOSIS
data = pred$x[,1]
# podział zbioru na treningowy i testowy
set.seed(123)
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train[,2:45] = scale(train[,2:45])
test[,2:45] = scale(test[,2:45])
library(MASS)
lnr <- lda(OVERALL_DIAGNOSIS~., train)
pred <- predict(lnr, train)
ldahist(data = pred$x[,1], g = train$OVERALL_DIAGNOSIS)
LD1_proj = pred$x
LD1 = pred$x
heart <- read.csv("heart.csv")
# podział zbioru na treningowy i testowy
set.seed(123)
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train[,2:45] = scale(train[,2:45])
test[,2:45] = scale(test[,2:45])
library(MASS)
lnr <- lda(OVERALL_DIAGNOSIS~., train)
class = train$OVERALL_DIAGNOSIS
pred <- predict(lnr, train)
ldahist(data = pred$x[,1], class)
dev.off()
library(ggplot2)
LD1 = pred$x
df = data.frame(LD1, class = as.factor(class))
ggplot(data = df)+geom_density(aes(LD1, fill = g), alpha = 0.1)
ggplot(data = df)+geom_density(aes(LD1, fill = class), alpha = 0.1)
heart <- read.csv("heart.csv")
# podział zbioru na treningowy i testowy
set.seed(123)
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train[,2:45] = scale(train[,2:45])
test[,2:45] = scale(test[,2:45])
library(MASS)
lnr <- lda(OVERALL_DIAGNOSIS~., train)
class = train$OVERALL_DIAGNOSIS
pred <- predict(lnr, train)
LD1 = pred$x
ldahist(data = LD1, class)
dev.off()
library(ggplot2)
df = data.frame(LD1, class = as.factor(class))
ggplot(data = df)+geom_density(aes(LD1, fill = class), alpha = 0.1)
# macierz pomyłek dla danych treningowych
p.train <- predict(lnr, train)$class
table_train <- table(predicted = p.train, Actual = train$OVERALL_DIAGNOSIS)
table_train
sum(diag(table_train)/sum(table_train))
# macierz pomyłek dla danych testowych
p.test <- predict(lnr, test)$class
table_test <- table(predicted = p.test, Actual = test$OVERALL_DIAGNOSIS)
table_test
sum(diag(table_test)/sum(table_test))
library(ROCR)
lnr
# REDUKCJA WYMIAROWOŚCI
data <- read.csv("heart.csv")
# Scale dataset
maxs = apply( data[,2:45], 2, max )
mins = apply( data[,2:45], 2, min )
dataset = as.data.frame( scale( data[,2:45], center = mins, scale = maxs - mins ) )
dataset = cbind( dataset, "class" = data$OVERALL_DIAGNOSIS )
# Split dataset
index = sample( 1:nrow(dataset), round(nrow(dataset)*0.7), replace = FALSE )
X_train = dataset[index,]
test = dataset[-index,]
new_X_train = as.matrix(X_train[,2:45])
new_X_train = as.data.frame(new_X_train)
new_X_train$class = X_train$class
library(neuralnet)
install.packages("neuralnet")
library(neuralnet)
n = names(new_X_train)
f = as.formula("normal+abnormal ~ LD1")
nn = neuralnet(f, new_X_train, hidden = 3, linear.output = FALSE, lifesign = "full", threshold = 0.02, stepmax = 1e6)
new_X_train
new_X_train = cbind( new_X_train, normal = new_X_train$class == "normal" )
new_X_train = cbind( new_X_train, abnormal = new_X_train$class == "abnormal" )
new_X_train = new_X_train[, !( names( new_X_train ) %in% c( "class" ) ) ]
n = names(new_X_train)
f = as.formula("normal+abnormal ~ LD1")
new_X_train
nn = neuralnet(f, new_X_train, hidden = 3, linear.output = FALSE, lifesign = "full", threshold = 0.02, stepmax = 1e6)
library(MASS)
library(ggplot2)
library(neuralnet)
heart <- read.csv("heart.csv")
# podział zbioru na treningowy i testowy
set.seed(123)
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train[,2:45] = scale(train[,2:45])
test[,2:45] = scale(test[,2:45])
lnr <- lda(OVERALL_DIAGNOSIS~., train)
lnr
# krzywa ROC dla danych testowych
p.test.posteriors <- as.data.frame(p.test.posteriors$posterior)
library(MASS)
library(ggplot2)
library(neuralnet)
heart <- read.csv("heart.csv")
# podział zbioru na treningowy i testowy
set.seed(123)
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train[,2:45] = scale(train[,2:45])
test[,2:45] = scale(test[,2:45])
lnr <- lda(OVERALL_DIAGNOSIS~., train)
lnr
# histogramy funkcji dyskryminacji dla poszczególnych klas
class = train$OVERALL_DIAGNOSIS
pred <- predict(lnr, train)
LD1 = pred$x
ldahist(data = LD1, class)
dev.off()
# funkcje gęstości wektora zmiennych dyskryminujących dla obu klas na jednym wykresie
df = data.frame(LD1, class = as.factor(class))
ggplot(data = df)+geom_density(aes(LD1, fill = class), alpha = 0.1)
dev.off()
# macierz pomyłek dla danych treningowych
p.train <- predict(lnr, train)$class
table_train <- table(predicted = p.train, Actual = train$OVERALL_DIAGNOSIS)
table_train
sum(diag(table_train)/sum(table_train))
# macierz pomyłek dla danych testowych
p.test <- predict(lnr, test)$class
table_test <- table(predicted = p.test, Actual = test$OVERALL_DIAGNOSIS)
table_test
sum(diag(table_test)/sum(table_test))
# krzywa ROC dla danych testowych
p.test.posteriors <- as.data.frame(p.test.posteriors$posterior)
# krzywa ROC dla danych testowych
p.test.posteriors <- as.data.frame(p.test$posterior)
p.test
p.test$posterior
# krzywa ROC dla danych testowych
p.test <- predict(lnr, test)
p.test
p.test.posteriors <- as.data.frame(p.test$posterior)
library(MASS)
library(ggplot2)
library(neuralnet)
heart <- read.csv("heart.csv")
# podział zbioru na treningowy i testowy
set.seed(123)
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train[,2:45] = scale(train[,2:45])
test[,2:45] = scale(test[,2:45])
lnr <- lda(OVERALL_DIAGNOSIS~., train)
lnr
# histogramy funkcji dyskryminacji dla poszczególnych klas
class = train$OVERALL_DIAGNOSIS
pred <- predict(lnr, train)
LD1 = pred$x
ldahist(data = LD1, class)
dev.off()
# funkcje gęstości wektora zmiennych dyskryminujących dla obu klas na jednym wykresie
df = data.frame(LD1, class = as.factor(class))
ggplot(data = df)+geom_density(aes(LD1, fill = class), alpha = 0.1)
dev.off()
# macierz pomyłek dla danych treningowych
p.train <- predict(lnr, train)$class
table_train <- table(predicted = p.train, Actual = train$OVERALL_DIAGNOSIS)
table_train
sum(diag(table_train)/sum(table_train))
# macierz pomyłek dla danych testowych
p.test <- predict(lnr, test)$class
table_test <- table(predicted = p.test, Actual = test$OVERALL_DIAGNOSIS)
table_test
sum(diag(table_test)/sum(table_test))
# krzywa ROC dla danych testowych
p.test <- predict(lnr, test)
p.test.posteriors <- as.data.frame(p.test$posterior)
pred <- prediction(p.test.posteriors[,2], test$OVERALL_DIAGNOSIS)
test$OVERALL_DIAGNOSIS
p.test.posteriors[,2]
lengths(test$OVERALL_DIAGNOSIS)
length(test$OVERALL_DIAGNOSIS)
length(p.test.posteriors[,2])
length(as.data.frame(test$OVERALL_DIAGNOSIS))
as.data.frame(test$OVERALL_DIAGNOSIS)
pred <- prediction(p.test.posteriors[,2], as.data.frame(test$OVERALL_DIAGNOSIS))
pred <- prediction(p.test.posteriors[,2], test["OVERALL_DIAGNOSIS"])
length(p.test.posteriors[,2])
p.test.posteriors[,2]
is.atomic(p.test.posteriors[,2])
is.atomic(test$OVERALL_DIAGNOSIS)
is.atomic(test["OVERALL_DIAGNOSIS"])
p.test.posteriors
# krzywa ROC dla danych testowych
plot(lnr)
library(MASS)
library(ggplot2)
library(neuralnet)
heart <- read.csv("heart.csv")
# podział zbioru na treningowy i testowy
set.seed(123)
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train[,2:45] = scale(train[,2:45])
test[,2:45] = scale(test[,2:45])
lnr <- lda(OVERALL_DIAGNOSIS~., train)
lnr
# histogramy funkcji dyskryminacji dla poszczególnych klas
class = train$OVERALL_DIAGNOSIS
pred <- predict(lnr)
LD1 = pred$x
ldahist(data = LD1, class)
dev.off()
ldahist(data = LD1, class)
# funkcje gęstości wektora zmiennych dyskryminujących dla obu klas na jednym wykresie
df = data.frame(LD1, class = as.factor(class))
ggplot(data = df)+geom_density(aes(LD1, fill = class), alpha = 0.1)
dev.off()
ggplot(data = df)+geom_density(aes(LD1, fill = class), alpha = 0.1)
library(MASS)
library(ggplot2)
library(neuralnet)
heart <- read.csv("heart.csv")
# podział zbioru na treningowy i testowy
set.seed(123)
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train[,2:45] = scale(train[,2:45])
test[,2:45] = scale(test[,2:45])
lnr <- lda(OVERALL_DIAGNOSIS~., train)
lnr
# histogramy funkcji dyskryminacji dla poszczególnych klas
class = train$OVERALL_DIAGNOSIS
pred <- predict(lnr, train)
LD1 = pred$x
ldahist(data = LD1, class)
dev.off()
# funkcje gęstości wektora zmiennych dyskryminujących dla obu klas na jednym wykresie
df = data.frame(LD1, class = as.factor(class))
ggplot(data = df)+geom_density(aes(LD1, fill = class), alpha = 0.1)
dev.off()
# macierz pomyłek dla danych treningowych
p.train <- predict(lnr, train)$class
table_train <- table(predicted = p.train, Actual = train$OVERALL_DIAGNOSIS)
table_train
sum(diag(table_train)/sum(table_train))
# macierz pomyłek dla danych testowych
p.test <- predict(lnr, test)$class
table_test <- table(predicted = p.test, Actual = test$OVERALL_DIAGNOSIS)
table_test
sum(diag(table_test)/sum(table_test))
# krzywa ROC dla danych testowych
p.test <- predict(lnr, test)
p.test.posteriors <- as.data.frame(p.test$posterior)
pred <- prediction(p.test.posteriors[,2], test$OVERALL_DIAGNOSIS)
is.atomic(test$OVERALL_DIAGNOSIS)
is.atomic(test["OVERALL_DIAGNOSIS"])
is.atomic(p.test.posteriors[,2])
p.test.posteriors[,2]
p.test.posteriors
p.test.posteriors[,2]
p.test.posteriors
p.test.posteriors["noramal"]
p.test.posteriors["normal"]
is.atomic(p.test.posteriors)
pred <- prediction(p.test.posteriors["normal"], test["OVERALL_DIAGNOSIS"])
p.test.posteriors["normal"]
library(MASS)
library(ggplot2)
library(neuralnet)
heart <- read.csv("heart.csv")
# podział zbioru na treningowy i testowy
set.seed(123)
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train[,2:45] = scale(train[,2:45])
test[,2:45] = scale(test[,2:45])
start_time <- Sys.time()
lnr <- lda(OVERALL_DIAGNOSIS~., train)
end_time <- Sys.time()
end_time - start_time
heart <- read.csv("heart.csv")
start_time <- Sys.time()
heart.pr <- prcomp(heart[c(2:length(colnames(heart)))], center = TRUE, scale = TRUE)
end_time <- Sys.time()
end_time - start_time
