source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
library("factoextra")
fviz_pca_ind(heart.pr, geom.ind = "point", pointshape = 21,
pointsize = 2,
fill.ind = heart$OVERALL_DIAGNOSIS,
col.ind = "black",
palette = "jco",
addEllipses = TRUE,
label = "var",
col.var = "black",
repel = TRUE,
legend.title = "Diagnoza") +
ggtitle("Dane z zaznaczonymi klasami") +
theme(plot.title = element_text(hjust = 0.5))
heart <- read.csv("heart.csv")
install.packages("factoextra")
heart.pr <- prcomp(heart[c(2:length(colnames(heart)))], center = TRUE, scale = TRUE)
# Analiza danych PCA
# Wczytanie danych
heart <- read.csv("heart.csv")
heart.pr <- prcomp(heart[c(2:length(colnames(heart)))], center = TRUE, scale = TRUE)
# wyznaczenie wariancji zawartej w dwóch największych składnikach wiodących
pc1_var <- as.double(summary(heart.pr)$importance[,1][2])
pc2_var <- as.double(summary(heart.pr)$importance[,2][2])
print(sprintf('Wariancja danych zawarta w pierwszym składniku wiodącym: %s%%', format(round(pc1_var, 2), nsmall = 2)))
print(sprintf('Wariancja danych zawarta w drugim składniku wiodącym: %s%%', format(round(pc2_var, 2), nsmall = 2)))
# Wykres wartości własnych dla 15 największych składników wiodących
pdf('docs/wartosci_wlasne.pdf')
screeplot(heart.pr, type = "l", npcs = 15, main = "Wykres wartości własnych dla 15 największych składników wiodących")
abline(h = 1, col="red", lty=5)
legend("topright", legend=c("Wartość własna = 1"),
col=c("red"), lty=5, cex=0.6)
dev.off()
# kumulacyjny wykres wariancji
cumpro <- cumsum(heart.pr$sdev^2 / sum(heart.pr$sdev^2))
pdf('docs/wariancja_kum.pdf')
plot(cumpro[0:45], xlab = "Składnik wiodący", ylab = "Wariancja wyjaśniona", main = "Kumulatywny wykres wariancji")
legend("topleft", legend=c("Cut-off @ PC6"),
col=c("blue"), lty=5, cex=0.6)
dev.off()
x_label <- sprintf('składnik wiodący 1: (%s%%)', format(round(pc1_var, 2), nsmall = 2))
y_label <- sprintf('składnik wiodący 2: (%s%%)', format(round(pc2_var, 2), nsmall = 2))
plot_title <- 'Dane po redukcji rozmiaru do dwóch wymiarów'
plot(heart.pr$x[,1],heart.pr$x[,2], xlab=x_label, ylab = y_label, main = plot_title)
# to wrzuććie do konsoli, ze skryptu mi się nie rysuje a ładne jest
library("factoextra")
pdf('docs/klasy.pdf')
fviz_pca_ind(heart.pr, geom.ind = "point", pointshape = 21,
pointsize = 2,
fill.ind = heart$OVERALL_DIAGNOSIS,
col.ind = "black",
palette = "jco",
addEllipses = TRUE,
label = "var",
col.var = "black",
repel = TRUE,
legend.title = "Diagnoza") +
ggtitle("Dane z zaznaczonymi klasami") +
theme(plot.title = element_text(hjust = 0.5))
dev.off()
# Analiza danych PCA
# Wczytanie danych
heart <- read.csv("heart.csv")
heart.pr <- prcomp(heart[c(2:length(colnames(heart)))], center = TRUE, scale = TRUE)
# wyznaczenie wariancji zawartej w dwóch największych składnikach wiodących
pc1_var <- as.double(summary(heart.pr)$importance[,1][2])
pc2_var <- as.double(summary(heart.pr)$importance[,2][2])
print(sprintf('Wariancja danych zawarta w pierwszym składniku wiodącym: %s%%', format(round(pc1_var, 2), nsmall = 2)))
print(sprintf('Wariancja danych zawarta w drugim składniku wiodącym: %s%%', format(round(pc2_var, 2), nsmall = 2)))
# Wykres wartości własnych dla 15 największych składników wiodących
pdf('docs/wartosci_wlasne.pdf')
screeplot(heart.pr, type = "l", npcs = 15, main = "Wykres wartości własnych dla 15 największych składników wiodących")
abline(h = 1, col="red", lty=5)
legend("topright", legend=c("Wartość własna = 1"),
col=c("red"), lty=5, cex=0.6)
dev.off()
# kumulacyjny wykres wariancji
cumpro <- cumsum(heart.pr$sdev^2 / sum(heart.pr$sdev^2))
pdf('docs/wariancja_kum.pdf')
plot(cumpro[0:45], xlab = "Składnik wiodący", ylab = "Wariancja wyjaśniona", main = "Kumulatywny wykres wariancji")
legend("topleft", legend=c("Cut-off @ PC6"),
col=c("blue"), lty=5, cex=0.6)
dev.off()
x_label <- sprintf('składnik wiodący 1: (%s%%)', format(round(pc1_var, 2), nsmall = 2))
y_label <- sprintf('składnik wiodący 2: (%s%%)', format(round(pc2_var, 2), nsmall = 2))
plot_title <- 'Dane po redukcji rozmiaru do dwóch wymiarów'
plot(heart.pr$x[,1],heart.pr$x[,2], xlab=x_label, ylab = y_label, main = plot_title)
# to wrzuććie do konsoli, ze skryptu mi się nie rysuje a ładne jest
# library("factoextra")
# pdf('docs/klasy.pdf')
# fviz_pca_ind(heart.pr, geom.ind = "point", pointshape = 21,
#              pointsize = 2,
#              fill.ind = heart$OVERALL_DIAGNOSIS,
#              col.ind = "black",
#              palette = "jco",
#              addEllipses = TRUE,
#              label = "var",
#              col.var = "black",
#              repel = TRUE,
#              legend.title = "Diagnoza") +
#   ggtitle("Dane z zaznaczonymi klasami") +
#   theme(plot.title = element_text(hjust = 0.5))
# dev.off()
heart <- read.csv("heart.csv")
heart.pr <- prcomp(heart[c(2:length(colnames(heart)))], center = TRUE, scale = TRUE)
# wyznaczenie wariancji zawartej w dwóch największych składnikach wiodących
pc1_var <- as.double(summary(heart.pr)$importance[,1][2])
pc2_var <- as.double(summary(heart.pr)$importance[,2][2])
print(sprintf('Wariancja danych zawarta w pierwszym składniku wiodącym: %s%%', format(round(pc1_var, 2), nsmall = 2)))
print(sprintf('Wariancja danych zawarta w drugim składniku wiodącym: %s%%', format(round(pc2_var, 2), nsmall = 2)))
# Wykres wartości własnych dla 15 największych składników wiodących
pdf('docs/wartosci_wlasne.pdf')
screeplot(heart.pr, type = "l", npcs = 15, main = "Wykres wartości własnych dla 15 największych składników wiodących")
abline(h = 1, col="red", lty=5)
legend("topright", legend=c("Wartość własna = 1"),
col=c("red"), lty=5, cex=0.6)
dev.off()
# kumulacyjny wykres wariancji
cumpro <- cumsum(heart.pr$sdev^2 / sum(heart.pr$sdev^2))
pdf('docs/wariancja_kum.pdf')
plot(cumpro[0:45], xlab = "Składnik wiodący", ylab = "Wariancja wyjaśniona", main = "Kumulatywny wykres wariancji")
legend("topleft", legend=c("Cut-off @ PC6"),
col=c("blue"), lty=5, cex=0.6)
dev.off()
# kumulacyjny wykres wariancji
cumpro <- cumsum(heart.pr$sdev^2 / sum(heart.pr$sdev^2))
pdf('docs/wariancja_kum.pdf')
plot(cumpro[0:45], xlab = "Składnik wiodący", ylab = "Wariancja wyjaśniona", main = "Kumulatywny wykres wariancji")
heart <- read.csv("heart.csv")
heart.pr <- prcomp(heart[c(2:length(colnames(heart)))], center = TRUE, scale = TRUE)
# wyznaczenie wariancji zawartej w dwóch największych składnikach wiodących
pc1_var <- as.double(summary(heart.pr)$importance[,1][2])
pc2_var <- as.double(summary(heart.pr)$importance[,2][2])
print(sprintf('Wariancja danych zawarta w pierwszym składniku wiodącym: %s%%', format(round(pc1_var, 2), nsmall = 2)))
print(sprintf('Wariancja danych zawarta w drugim składniku wiodącym: %s%%', format(round(pc2_var, 2), nsmall = 2)))
# kumulacyjny wykres wariancji
cumpro <- cumsum(heart.pr$sdev^2 / sum(heart.pr$sdev^2))
pdf('docs/wariancja_kum.pdf')
plot(cumpro[0:45], xlab = "Składnik wiodący", ylab = "Wariancja wyjaśniona", main = "Kumulatywny wykres wariancji")
legend("topleft", legend=c("Cut-off @ PC6"),
col=c("blue"), lty=5, cex=0.6)
dev.off()
warnings()
x_label <- sprintf('składnik wiodący 1: (%s%%)', format(round(pc1_var, 2), nsmall = 2))
y_label <- sprintf('składnik wiodący 2: (%s%%)', format(round(pc2_var, 2), nsmall = 2))
plot_title <- 'Dane po redukcji rozmiaru do dwóch wymiarów'
plot(heart.pr$x[,1],heart.pr$x[,2], xlab=x_label, ylab = y_label, main = plot_title)
heart <- read.csv("heart.csv")
head(heart, 3)
head(heart, 1)
# head(heart, 1)
str(heart)
# head(heart, 1)
str(heart)
head(heart, 1)
library(MASS)
# podział zbioru na treningowy i testowy
heart <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
heart <- read.csv("heart.csv")
# podział zbioru na treningowy i testowy
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1]
test <- heart[sampled == 2]
library(MASS)
heart <- read.csv("heart.csv")
# podział zbioru na treningowy i testowy
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
heart <- read.csv("heart.csv")
# podział zbioru na treningowy i testowy
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
library(MASS)
lnr <- lda(heart~., train)
# head(heart, 1)
# str(heart)
summary(heart)
# podział zbioru na treningowy i testowy
install.packages('caTools')
library(caTools)
heart <- read.csv("heart.csv")
# head(heart, 1)
# str(heart)
summary(heart)
# podział zbioru na treningowy i testowy
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train[-45] = scale(train[-45])
test[-45] = scale(test[-45])
heart <- read.csv("heart.csv")
# head(heart, 1)
# str(heart)
summary(heart)
# podział zbioru na treningowy i testowy
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train
test
train[,2:45] = scale(train[,2:45])
test[,2:45] = scale(test[,2:45])
library(MASS)
lnr <- lda(Heart~., train)
lnr
heart <- read.csv("heart.csv")
# head(heart, 1)
# str(heart)
summary(heart)
# podział zbioru na treningowy i testowy
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train[,2:45] = scale(train[,2:45])
test[,2:45] = scale(test[,2:45])
library(MASS)
lnr <- lda(heart~., train)
lnr
heart <- read.csv("heart.csv")
# head(heart, 1)
# str(heart)
summary(heart)
# podział zbioru na treningowy i testowy
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train[,2:45] = scale(train[,2:45])
test[,2:45] = scale(test[,2:45])
library(MASS)
lnr <- lda('OVERALL DIAGNOSIS'~., train)
lnr
install.packages('caTools')
heart <- read.csv("heart.csv")
# head(heart, 1)
# str(heart)
summary(heart)
# podział zbioru na treningowy i testowy
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train[,2:45] = scale(train[,2:45])
test[,2:45] = scale(test[,2:45])
View(train)
heart <- read.csv("heart.csv")
# head(heart, 1)
# str(heart)
summary(heart)
# podział zbioru na treningowy i testowy
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train[,2:45] = scale(train[,2:45])
test[,2:45] = scale(test[,2:45])
library(MASS)
lnr <- lda(OVERALL_DIAGNOSIS~., train)
lnr
lnr
heart <- read.csv("heart.csv")
# head(heart, 1)
# str(heart)
summary(heart)
# podział zbioru na treningowy i testowy
set.seed(123)
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train[,2:45] = scale(train[,2:45])
test[,2:45] = scale(test[,2:45])
library(MASS)
lnr <- lda(OVERALL_DIAGNOSIS~., train)
lnr
lnr$prior
lnr$counts
pred <- predict(lnr, train)
ldahist(data = pred$x[,1], g = train$OVERALL_DIAGNOSIS)
library(devtools)
library(ggord)
install_github("fawda123/ggord")
# install_github("fawda123/ggord")
library(ggord)
ggord(lnr, train$OVERALL_DIAGNOSIS, ylim = c(-5, 5))
ggord(lnr, train$OVERALL_DIAGNOSIS)
heart <- read.csv("heart.csv")
# head(heart, 1)
# str(heart)
summary(heart)
heart <- read.csv("heart.csv")
# podział zbioru na treningowy i testowy
set.seed(123)
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train[,2:45] = scale(train[,2:45])
test[,2:45] = scale(test[,2:45])
library(MASS)
lnr <- lda(OVERALL_DIAGNOSIS~., train)
pred <- predict(lnr, train)
ldahist(data = pred$x[,1], g = train$OVERALL_DIAGNOSIS)
ldahist(data = pred$x[,1], g = train$OVERALL_DIAGNOSIS)
ldahist(data = pred$x[,1], g = train$OVERALL_DIAGNOSIS)
g = train$OVERALL_DIAGNOSIS
data = pred$x[,1]
library(devtools)
# install_github("fawda123/ggord")
library(ggord)
ggord(lnr, train$OVERALL_DIAGNOSIS)
heart <- read.csv("heart.csv")
# podział zbioru na treningowy i testowy
set.seed(123)
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train[,2:45] = scale(train[,2:45])
test[,2:45] = scale(test[,2:45])
library(MASS)
lnr <- lda(OVERALL_DIAGNOSIS~., train)
pred <- predict(lnr, train)
ldahist(data = pred$x[,1], g = train$OVERALL_DIAGNOSIS)
dev.off()
heart <- read.csv("heart.csv")
heart.pr <- prcomp(heart[c(2:length(colnames(heart)))], center = TRUE, scale = TRUE)
# wyznaczenie wariancji zawartej w dwóch największych składnikach wiodących
pc1_var <- as.double(summary(heart.pr)$importance[,1][2])
pc2_var <- as.double(summary(heart.pr)$importance[,2][2])
print(sprintf('Wariancja danych zawarta w pierwszym składniku wiodącym: %s%%', format(round(pc1_var, 2), nsmall = 2)))
print(sprintf('Wariancja danych zawarta w drugim składniku wiodącym: %s%%', format(round(pc2_var, 2), nsmall = 2)))
# Wykres wartości własnych dla 15 największych składników wiodących
pdf('docs/wartosci_wlasne.pdf')
screeplot(heart.pr, type = "l", npcs = 15, main = "Wykres wartości własnych dla 15 największych składników wiodących")
abline(h = 1, col="red", lty=5)
legend("topright", legend=c("Wartość własna = 1"),
col=c("red"), lty=5, cex=0.6)
dev.off()
# kumulacyjny wykres wariancji
cumpro <- cumsum(heart.pr$sdev^2 / sum(heart.pr$sdev^2))
pdf('docs/wariancja_kum.pdf')
plot(cumpro[0:45], xlab = "Składnik wiodący", ylab = "Wariancja wyjaśniona", main = "Kumulatywny wykres wariancji")
legend("topleft", legend=c("Cut-off @ PC6"),
col=c("blue"), lty=5, cex=0.6)
dev.off()
x_label <- sprintf('składnik wiodący 1: (%s%%)', format(round(pc1_var, 2), nsmall = 2))
y_label <- sprintf('składnik wiodący 2: (%s%%)', format(round(pc2_var, 2), nsmall = 2))
plot_title <- 'Dane po redukcji rozmiaru do dwóch wymiarów'
plot(heart.pr$x[,1],heart.pr$x[,2], xlab=x_label, ylab = y_label, main = plot_title)
# to wrzuććie do konsoli, ze skryptu mi się nie rysuje a ładne jest
library("factoextra")
pdf('docs/klasy.pdf')
fviz_pca_ind(heart.pr, geom.ind = "point", pointshape = 21,
pointsize = 2,
fill.ind = heart$OVERALL_DIAGNOSIS,
col.ind = "black",
palette = "jco",
addEllipses = TRUE,
label = "var",
col.var = "black",
repel = TRUE,
legend.title = "Diagnoza") +
ggtitle("Dane z zaznaczonymi klasami") +
theme(plot.title = element_text(hjust = 0.5))
dev.off()
source('~/Desktop/wizualizacja_PCA_LDA/proj/analiza_PCA.R')
heart <- read.csv("heart.csv")
# podział zbioru na treningowy i testowy
set.seed(123)
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train[,2:45] = scale(train[,2:45])
test[,2:45] = scale(test[,2:45])
library(MASS)
lnr <- lda(OVERALL_DIAGNOSIS~., train)
pred <- predict(lnr, train)
ldahist(data = pred$x[,1], g = train$OVERALL_DIAGNOSIS)
dev.off()
g = train$OVERALL_DIAGNOSIS
data = pred$x[,1]
library(devtools)
# install_github("fawda123/ggord")
library(ggord)
ggord(lnr, train$OVERALL_DIAGNOSIS)
ldahist(data = pred$x[,1], g = train$OVERALL_DIAGNOSIS)
heart <- read.csv("heart.csv")
# podział zbioru na treningowy i testowy
set.seed(123)
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
library(MASS)
lnr <- lda(OVERALL_DIAGNOSIS~., train)
pred <- predict(lnr, train)
ldahist(data = pred$x[,1], g = train$OVERALL_DIAGNOSIS)
dev.off()
g = train$OVERALL_DIAGNOSIS
data = pred$x[,1]
# podział zbioru na treningowy i testowy
set.seed(123)
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
library(MASS)
lnr <- lda(OVERALL_DIAGNOSIS~., train)
pred <- predict(lnr, train)
ldahist(data = pred$x[,1], g = train$OVERALL_DIAGNOSIS)
dev.off()
g = train$OVERALL_DIAGNOSIS
data = pred$x[,1]
# podział zbioru na treningowy i testowy
set.seed(123)
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train[,2:45] = scale(train[,2:45])
test[,2:45] = scale(test[,2:45])
library(MASS)
lnr <- lda(OVERALL_DIAGNOSIS~., train)
pred <- predict(lnr, train)
ldahist(data = pred$x[,1], g = train$OVERALL_DIAGNOSIS)
LD1_proj = pred$x
LD1 = pred$x
heart <- read.csv("heart.csv")
# podział zbioru na treningowy i testowy
set.seed(123)
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train[,2:45] = scale(train[,2:45])
test[,2:45] = scale(test[,2:45])
library(MASS)
lnr <- lda(OVERALL_DIAGNOSIS~., train)
class = train$OVERALL_DIAGNOSIS
pred <- predict(lnr, train)
ldahist(data = pred$x[,1], class)
dev.off()
library(ggplot2)
LD1 = pred$x
df = data.frame(LD1, class = as.factor(class))
ggplot(data = df)+geom_density(aes(LD1, fill = g), alpha = 0.1)
ggplot(data = df)+geom_density(aes(LD1, fill = class), alpha = 0.1)
heart <- read.csv("heart.csv")
# podział zbioru na treningowy i testowy
set.seed(123)
sampled <- sample(2, nrow(heart), replace = TRUE, prob = c(0.75, 0.25))
train <- heart[sampled == 1,]
test <- heart[sampled == 2,]
# skalowanie
train[,2:45] = scale(train[,2:45])
test[,2:45] = scale(test[,2:45])
library(MASS)
lnr <- lda(OVERALL_DIAGNOSIS~., train)
class = train$OVERALL_DIAGNOSIS
pred <- predict(lnr, train)
LD1 = pred$x
ldahist(data = LD1, class)
dev.off()
library(ggplot2)
df = data.frame(LD1, class = as.factor(class))
ggplot(data = df)+geom_density(aes(LD1, fill = class), alpha = 0.1)
# macierz pomyłek dla danych treningowych
p.train <- predict(lnr, train)$class
table_train <- table(predicted = p.train, Actual = train$OVERALL_DIAGNOSIS)
table_train
sum(diag(table_train)/sum(table_train))
# macierz pomyłek dla danych testowych
p.test <- predict(lnr, test)$class
table_test <- table(predicted = p.test, Actual = test$OVERALL_DIAGNOSIS)
table_test
sum(diag(table_test)/sum(table_test))
library(ROCR)
lnr
# REDUKCJA WYMIAROWOŚCI
data <- read.csv("heart.csv")
# Scale dataset
maxs = apply( data[,2:45], 2, max )
mins = apply( data[,2:45], 2, min )
dataset = as.data.frame( scale( data[,2:45], center = mins, scale = maxs - mins ) )
dataset = cbind( dataset, "class" = data$OVERALL_DIAGNOSIS )
# Split dataset
index = sample( 1:nrow(dataset), round(nrow(dataset)*0.7), replace = FALSE )
X_train = dataset[index,]
test = dataset[-index,]
new_X_train = as.matrix(X_train[,2:45])
new_X_train = as.data.frame(new_X_train)
new_X_train$class = X_train$class
library(neuralnet)
install.packages("neuralnet")
library(neuralnet)
n = names(new_X_train)
f = as.formula("normal+abnormal ~ LD1")
nn = neuralnet(f, new_X_train, hidden = 3, linear.output = FALSE, lifesign = "full", threshold = 0.02, stepmax = 1e6)
new_X_train
new_X_train = cbind( new_X_train, normal = new_X_train$class == "normal" )
new_X_train = cbind( new_X_train, abnormal = new_X_train$class == "abnormal" )
new_X_train = new_X_train[, !( names( new_X_train ) %in% c( "class" ) ) ]
n = names(new_X_train)
f = as.formula("normal+abnormal ~ LD1")
new_X_train
nn = neuralnet(f, new_X_train, hidden = 3, linear.output = FALSE, lifesign = "full", threshold = 0.02, stepmax = 1e6)
